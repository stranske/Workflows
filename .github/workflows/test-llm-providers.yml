# Test LLM Providers - Manual workflow to verify API keys work
name: Test LLM Providers

on:
  workflow_dispatch:
    inputs:
      provider:
        description: 'Provider to test'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - github-models
          - openai

permissions:
  contents: read
  models: read

jobs:
  test-providers:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r tools/requirements.txt

      - name: Check provider availability
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -c "
          from tools.llm_provider import check_providers
          print('Provider availability:')
          for name, available in check_providers().items():
              status = '✓' if available else '✗'
              print(f'  {status} {name}')
          "

      - name: Test GitHub Models
        if: inputs.provider == 'all' || inputs.provider == 'github-models'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "Testing GitHub Models provider..."
          python -c "
          from tools.llm_provider import get_llm_provider
          provider = get_llm_provider(force_provider='github-models')
          result = provider.analyze_completion(
              session_output='I fixed the bug in auth.py. Tests pass now.',
              tasks=['Fix authentication bug'],
          )
          print(f'✅ GitHub Models: {result.provider_used}, confidence={result.confidence:.0%}')
          print(f'   Completed: {result.completed_tasks}')
          "

      - name: Test OpenAI
        if: inputs.provider == 'all' || inputs.provider == 'openai'
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          echo "Testing OpenAI provider..."
          python -c "
          from tools.llm_provider import get_llm_provider
          provider = get_llm_provider(force_provider='openai')
          result = provider.analyze_completion(
              session_output='I fixed the bug in auth.py. Tests pass now.',
              tasks=['Fix authentication bug'],
          )
          print(f'✅ OpenAI: {result.provider_used}, confidence={result.confidence:.0%}')
          print(f'   Completed: {result.completed_tasks}')
          "

      - name: Summary
        run: |
          echo "## LLM Provider Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Provider test completed successfully! ✅" >> $GITHUB_STEP_SUMMARY
