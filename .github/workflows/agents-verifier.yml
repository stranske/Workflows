name: Agents Verifier

on:
  pull_request:
    types:
      - closed
  push:
    branches:
      - main

permissions:
  contents: read
  pull-requests: read
  issues: write

jobs:
  verifier:
    name: Run post-merge verifier
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Build verifier context
        id: context
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const { buildVerifierContext } = require('./.github/scripts/agents_verifier_context.js');
            await buildVerifierContext({ github, context, core });

      - name: Stop when verifier is skipped
        if: steps.context.outputs.should_run != 'true'
        run: |
          echo "Verifier skipped: ${{ steps.context.outputs.skip_reason || 'no reason provided' }}"

      - name: Prepare verifier prompt
        if: steps.context.outputs.should_run == 'true'
        id: prepare
        run: |
          set -euo pipefail
          base_prompt=".github/codex/prompts/verifier_acceptance_check.md"
          context_file="${{ steps.context.outputs.context_path }}"
          combined="verifier-prompt.md"
          if [ ! -f "$base_prompt" ]; then
            echo "::error::Base prompt file missing: $base_prompt"
            exit 1
          fi
          if [ ! -f "$context_file" ]; then
            echo "::error::Context file missing: $context_file"
            exit 1
          fi
          {
            cat "$base_prompt"
            printf '\n\n---\n\n'
            cat "$context_file"
          } > "$combined"
          echo "prompt_file=$combined" >> "$GITHUB_OUTPUT"

      - name: Setup Codex auth
        if: steps.context.outputs.should_run == 'true'
        env:
          CODEX_AUTH_JSON: ${{ secrets.CODEX_AUTH_JSON }}
        run: |
          set -euo pipefail
          mkdir -p ~/.codex
          echo "$CODEX_AUTH_JSON" > ~/.codex/auth.json
          chmod 600 ~/.codex/auth.json
          echo "Codex auth configured from CODEX_AUTH_JSON secret"

      - name: Run verifier
        id: codex
        if: steps.context.outputs.should_run == 'true'
        uses: openai/codex-action@v1
        with:
          # Auth is pre-configured via ~/.codex/auth.json from CODEX_AUTH_JSON secret
          openai-api-key: 'auth-via-codex-auth-json'
          prompt-file: ${{ steps.prepare.outputs.prompt_file }}
          output-file: codex-output.md
          sandbox: read-only
          safety-strategy: drop-sudo
          codex-args: --mode verifier

      - name: Parse verifier verdict
        id: verdict
        if: steps.context.outputs.should_run == 'true'
        run: |
          set -euo pipefail
          verdict="unknown"
          output_file="codex-output.md"
          if [ -f "$output_file" ]; then
            if grep -qiE 'verdict:\\s*fail' "$output_file"; then
              verdict="fail"
            elif grep -qiE 'verdict:\\s*pass' "$output_file"; then
              verdict="pass"
            fi
          fi
          echo "verdict=$verdict" >> "$GITHUB_OUTPUT"

      - name: Open follow-up issue on verifier failure
        id: failure_issue
        if: steps.context.outputs.should_run == 'true' && steps.verdict.outputs.verdict == 'fail'
        uses: actions/github-script@v7
        env:
          PR_URL: ${{ steps.context.outputs.pr_html_url }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = Number('${{ steps.context.outputs.pr_number }}') || null;
            const issueNumbers = JSON.parse('${{ steps.context.outputs.issue_numbers || '[]' }}');
            const title = prNumber
              ? `Verifier failure for PR #${prNumber}`
              : 'Verifier failure on merged commit';
            const lines = [];
            if (process.env.PR_URL) {
              lines.push(`Source pull request: ${process.env.PR_URL}`);
            }
            if (Array.isArray(issueNumbers) && issueNumbers.length) {
              lines.push(`Linked issues: ${issueNumbers.map((n) => `#${n}`).join(', ')}`);
            }
            lines.push('');
            lines.push('## Verifier output');
            lines.push('');
            lines.push('```');
            lines.push(require('fs').readFileSync('codex-output.md', 'utf8'));
            lines.push('```');
            lines.push('');
            lines.push('- [ ] Re-run verifier after addressing the failures.');
            const body = lines.join('\n');
            const { data: issue } = await github.rest.issues.create({
              ...context.repo,
              title,
              body,
              labels: ['agent:codex'],
            });
            core.setOutput('issue_number', issue?.number ? String(issue.number) : '');

      - name: Collect verifier metrics
        if: always()
        id: collect_metrics
        env:
          SHOULD_RUN: ${{ steps.context.outputs.should_run }}
          PR_NUMBER: ${{ steps.context.outputs.pr_number }}
          VERDICT: ${{ steps.verdict.outputs.verdict }}
          CONTEXT_PATH: ${{ steps.context.outputs.context_path }}
          CODEX_OUTPUT: codex-output.md
          ISSUE_NUMBER: ${{ steps.failure_issue.outputs.issue_number }}
          SKIP_REASON: ${{ steps.context.outputs.skip_reason }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from pathlib import Path
          from datetime import datetime, timezone

          def count_checkboxes(text: str) -> int:
              return sum(1 for line in text.splitlines() if re.match(r"^- \\[[ xX]\\]", line.strip()))

          should_run = (os.environ.get("SHOULD_RUN") or "").lower() == "true"
          pr_number = int(os.environ.get("PR_NUMBER") or 0)
          verdict = os.environ.get("VERDICT") or "unknown"
          context_path = Path(os.environ.get("CONTEXT_PATH") or "")
          skip_reason = os.environ.get("SKIP_REASON") or ""
          issue_number = os.environ.get("ISSUE_NUMBER") or ""

          if not should_run:
              verdict = "skipped"

          issues_created = 1 if issue_number else 0
          issue_numbers = [issue_number] if issue_number else []

          acceptance_criteria_count = 0
          if context_path.is_file():
              acceptance_criteria_count = count_checkboxes(context_path.read_text(encoding="utf-8"))

          checks_run = 0
          codex_output_path = Path(os.environ.get("CODEX_OUTPUT") or "")
          if codex_output_path.is_file():
              content = codex_output_path.read_text(encoding="utf-8")
              checks_run = sum(1 for line in content.splitlines()[1:] if line.lstrip().startswith("- "))

          metrics = {
              "pr_number": pr_number,
              "verdict": verdict,
              "issues_created": issues_created,
              "issue_numbers": issue_numbers,
              "acceptance_criteria_count": acceptance_criteria_count,
              "checks_run": checks_run,
              "skip_reason": skip_reason,
              "recorded_at": datetime.now(timezone.utc).isoformat(),
          }

          print(json.dumps(metrics, indent=2))
          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as fp:
              fp.write(f"metrics_json={json.dumps(metrics)}\n")
          PY

      - name: Write verifier summary
        if: always()
        env:
          METRICS_JSON: ${{ steps.collect_metrics.outputs.metrics_json }}
        run: |
          set -euo pipefail
          if [ -z "${METRICS_JSON:-}" ]; then
            echo "No verifier metrics captured; skipping summary."
            exit 0
          fi
          python - <<'PY'
          import json
          import os

          metrics = json.loads(os.environ["METRICS_JSON"])
          order = [
              "pr_number",
              "verdict",
              "issues_created",
              "issue_numbers",
              "acceptance_criteria_count",
              "checks_run",
              "skip_reason",
              "recorded_at",
          ]

          lines = ["## Verifier metrics", ""] + ["| Field | Value |", "| --- | --- |"]
          for key in order:
              value = metrics.get(key, "")
              lines.append(f"| {key} | `{value}` |")

          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          if summary_path:
              with open(summary_path, "a", encoding="utf-8") as fp:
                  fp.write("\n".join(lines) + "\n")

          out_path = "verifier-metrics.ndjson"
          with open(out_path, "a", encoding="utf-8") as fp:
              fp.write(json.dumps(metrics) + "\n")
          print(f"Wrote metrics to {out_path}")
          PY

      - name: Upload verifier metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: agents-verifier-metrics
          path: verifier-metrics.ndjson
          retention-days: 30
