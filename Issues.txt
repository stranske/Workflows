1. Replace PATs with a GitHub App installation token across orchestrator + keepalive workflows (token-only automation)

   Context / problem:
   - Current orchestration depends on PATs and/or mixed identities, which is fragile and painful to maintain.
   - GitHub Actions has recursion protection: pushes/labels/comments made with GITHUB_TOKEN generally will NOT trigger other workflows.
   - A GitHub App installation token is the cleanest way to get predictable “workflow triggers workflow” behavior without tying everything to a human PAT.

   Goal:
   - Create a GitHub App (single org/user app) that can be installed on your repos.
   - Mint short-lived installation tokens inside workflows.
   - Replace all PAT usage in orchestrator + keepalive + dispatch workflows with the App token.

   Implementation notes:
   - See docs: docs/automation-system-implementation.md (new file in this PR)
   - Use actions/create-github-app-token@v1 to mint the installation token
   - Use that token for:
     - actions/checkout token
     - gh CLI (GH_TOKEN)
     - REST API calls in scripts
     - any git push / branch updates that must trigger downstream workflows

   Tasks:
   - Create GitHub App (UI, not code): name it "agents-workflows-bot" (or similar)
   - Set App permissions (minimal but sufficient):
         - Contents: Read & write
         - Pull requests: Read & write
         - Issues: Read & write
         - Actions: Read & write (for dispatching / reading runs)
         - Metadata: Read-only
   - Install the App on: Workflows, Workflows-Integration-Tests, Travel-Plan-Permission, Portable-Alpha-Extension-Model, Trend_Model_Project
   - Add secrets to Workflows repo (or org secrets):
         - WORKFLOWS_APP_ID
         - WORKFLOWS_APP_PRIVATE_KEY  (the PEM contents)
   - Update all workflows that currently use PATs to:
         - mint app token
         - export GH_TOKEN to that token
         - (optional) checkout using that token so git push is clean
   - Add a “compat mode” fallback (temporarily) so you can flip back to PAT if needed during rollout

   Acceptance criteria:
   - No workflow in Workflows repo requires a PAT for:
         - labeling PRs/issues
         - creating comments
         - pushing commits to PR branches
         - dispatching workflows
   - A commit pushed by the bot identity reliably triggers the Gate workflow (no “dead loop”).
   - Secrets inventory is reduced: only App ID + private key (and OPENAI_API_KEY) are required for the automation system.

   Rollout / safety:
   - Roll out in Workflows-Integration-Tests first, then Workflows, then consumer repos.
   - Add CODEOWNERS for .github/workflows/** and .github/scripts/** so this can’t get silently corrupted later.

2. Create a reusable “codex-run” workflow wrapper for openai/codex-action@v1 (dependency bootstrap + safe prompting + commit/push)

   Context / problem:
   - You need Codex to run in GitHub Actions so iteration happens on the PR branch without manual “Update Branch” clicks in the Codex website.
   - You also want a single, predictable place to:
     - install dependencies (pip, node, etc)
     - pin sandbox / safety settings
     - standardize prompts
     - standardize commit + push behavior
     - collect outputs (final-message, output-file)

   Goal:
   - Add a reusable workflow in Workflows repo:
     - .github/workflows/reusable-codex-run.yml
   - It should run Codex with a prompt-file, then commit + push any changes to the same PR branch.

   Tasks:
   - Create reusable workflow with inputs:
         - prompt_file (path in repo)
         - mode (keepalive | autofix | verifier)
         - pr_number (optional, for comments)
         - max_runtime_minutes
         - sandbox (workspace-write by default)
         - safety_strategy (drop-sudo by default)
         - codex_args (optional)
   - Steps:
         - mint GitHub App token
         - checkout with token + fetch-depth: 0
         - set up runtime(s) and install deps (Python + pip cache at minimum)
         - run openai/codex-action@v1 with:
             openai-api-key: ${{ secrets.OPENAI_API_KEY }}
             prompt-file: <input>
             output-file: codex-output.md
             safety-strategy: drop-sudo
             sandbox: workspace-write
         - commit + push changes (only if git diff is non-empty)
         - upload codex-output.md as artifact
   - Create prompt files directory:
         - .github/codex/prompts/
         - keepalive_next_task.md
         - autofix_from_ci_failure.md
         - verifier_acceptance_check.md

   Acceptance criteria:
   - Reusable workflow can be called by keepalive/autofix/verifier workflows.
   - It pushes commits to the PR branch under the GitHub App identity.
   - Gate workflow triggers on those pushes.

3. Keepalive v2: turn “keepalive” into a real closed loop (Gate workflow_run -> Codex -> commit -> Gate -> repeat)

   Context / problem:
   - Current keepalive does one round, then you manually click “Update Branch” (or create/merge a branch) in the Codex website to continue.
   - That manual step is the entire reason this doesn’t behave like a loop.

   Goal:
   - Implement a loop that is entirely GitHub-native:
     - Gate finishes -> if tasks remain -> Codex runs next task -> commit/push -> Gate reruns -> repeat
   - No Codex website clicking.

   Requirements:
   - Single label policy:
     - Only label needed: agent:codex
     - keepalive and autofix behavior controlled by a config block in PR body (written by Agents 63 bootstrap) or a repo-level default.

   Tasks:
   - Add a new workflow: .github/workflows/agents-keepalive-loop.yml
   - Trigger conditions:
         - on: workflow_run (workflows: ["Gate"], types: [completed])
         - optional: on pull_request labeled (agent:codex) to “start the loop”
   - Determine PR number from workflow_run payload
   - Fetch PR body and parse:
         - task list checkboxes
         - keepalive_enabled flag
         - autofix_enabled flag
         - iteration counter + max iterations
   - If Gate conclusion == success AND keepalive enabled AND tasks remain:
         - call reusable-codex-run with prompt keepalive_next_task.md
         - commit/push changes
         - post/update a single “Keepalive summary” comment (update-in-place)
   - Use concurrency:
         - group: keepalive-${{ pr_number }}
         - cancel-in-progress: false
   - Stop conditions:
         - tasks complete (all boxes checked) OR max iterations hit OR repeated identical failure
         - when stopping, add a label like needs-human (or post a loud comment) and exit

   Acceptance criteria:
   - A PR with a tasklist can advance multiple iterations without any manual Codex website interaction.
   - Gate green + tasks remaining reliably triggers the next iteration.
   - The loop stops safely and noisily when it can’t proceed.

4. Autofix v2: unify “autofix” into the same agent loop (handle mypy + pytest + more, not just black/ruff)

   Context / problem:
   - You already have a lint-only autofix (black/format/etc).
   - You want the same concept for:
     - mypy errors
     - failing pytest tests
     - “harder” lint issues
     - failures in tools/ and scripts/ (not just src/ and tests/)

   Goal:
   - If Gate fails, the system should attempt a bounded Codex-driven fix loop, pushing fixes onto the SAME PR branch.
   - Still one label: agent:codex (no second label).

   Tasks:
   - Add/extend workflow logic:
         - on workflow_run Gate completed
         - if conclusion != success AND autofix enabled:
             - build a prompt containing failure summary (mypy/pytest output, failing step names)
             - call reusable-codex-run with prompt autofix_from_ci_failure.md
             - commit/push fixes
   - Ensure the fix scope includes:
         - src/, tests/, tools/, scripts/ (and any repo-specific folders where checks commonly fail)
   - Ensure dependencies are installed BEFORE Codex runs:
         - python -m pip install -r requirements.txt
         - install dev deps needed for mypy/pytest
   - Bounded attempts:
         - max 2-4 iterations before stopping
         - add needs-human + summarize failures if still red

   Acceptance criteria:
   - When Gate fails due to mypy or pytest, the loop attempts an autofix and pushes a commit.
   - If it cannot fix after N tries, it stops and produces a human-readable failure summary.

5. Add a post-merge verifier workflow to check acceptance criteria and auto-create follow-up issues when agents “declare victory” incorrectly

   Context / problem:
   - Agents often claim acceptance criteria are met when they are not.
   - You want an automated verification pass after merge, and a fast path to restart the pipeline if needed.

   Goal:
   - On PR merge, run a verifier that:
     - checks acceptance criteria against the merged codebase
     - runs the important commands (tests/typecheck)
     - produces an explicit verdict
   - If NOT met, automatically open a follow-up GitHub issue with:
     - what failed
     - what to do next
     - link to the merged PR + original issue

   Tasks:
   - New workflow: .github/workflows/agents-verifier.yml
         - on: pull_request closed (merged == true) OR push to main with PR context
   - Fetch linked issue(s) and acceptance criteria (from PR body or issue reference)
   - Run Codex in “verifier” mode using prompt-file verifier_acceptance_check.md
   - If verifier fails:
         - open a new GitHub Issue automatically with the failure report
         - optionally label it agent:codex (if you want it to enter the pipeline immediately)

   Acceptance criteria:
   - A deliberately “bad” merged PR triggers a verifier failure + follow-up issue.
   - A good merged PR posts a clean verifier report and does not open issues.

6. Add minimal GitHub Environments + CODEOWNERS gates for sensitive automation (without constantly interrupting the loop)

   Goal:
   - Use GitHub Environments to add approvals only where they matter:
     - minting high-privilege tokens
     - modifying workflows
     - pushing to protected branches
   - Keep PR-branch iteration unblocked.

   Tasks:
   - Add CODEOWNERS:
         - .github/workflows/** requires review
         - .github/scripts/** requires review
         - docs/** optional review
   - Create environment(s):
         - agent-high-privilege (required reviewers: maintainers)
         - agent-standard (no approval)
   - Update workflows so that:
         - PR-branch keepalive/autofix uses agent-standard (no constant approvals)
         - anything touching default branch or workflows uses agent-high-privilege

   Acceptance criteria:
   - Keepalive/autofix can iterate without repeated approvals.
   - Any change to workflows requires explicit human review.

7. Prompt injection hardening for issue/PR-driven agents (beyond “human approves Issues.txt”)

   Context:
   - You’re right: a single approval step helps, but it’s not the whole story.
   - PR bodies, issue comments, and commit messages are all attacker-controlled text in many scenarios.

   Goal:
   - Ensure Codex is only fed curated prompt content and sanitized task context.

   Tasks:
   - Restrict triggers:
         - do not run agent workflows on forked PRs
         - avoid pull_request_target unless absolutely necessary
   - Ensure prompts are repo-owned:
         - use prompt-file from .github/codex/prompts/
         - build a small “context appendix” file that includes sanitized task text
   - Add allowlists:
         - allow-users / allow-bots in codex-action config
         - only repo collaborators can trigger
   - Add denylist behaviors:
         - Codex should not edit .github/workflows/** unless a special environment-approved mode is enabled
         - Codex should not touch secrets or tokens (explicit instruction + sandbox limits)
   - Add logging + red flags:
         - if prompt contains “ignore previous”, HTML comments, base64 blobs, etc, stop and require human

   Acceptance criteria:
   - Malicious-looking issue text does not get passed verbatim into Codex execution.
   - Agent workflows only run for trusted actors and trusted events.
8. Add comprehensive test coverage for new keepalive and agent loop scripts

   Context / problem:
   - The recently added agent scripts have no unit test coverage:
     - keepalive_loop.js (435 lines) - core loop evaluation logic
     - keepalive_state.js (209 lines) - state persistence and comment formatting
     - agents_verifier_context.js (207 lines) - post-merge verification context builder
   - These scripts contain critical logic for checkbox parsing, iteration counting, stop condition evaluation, and PR state management.
   - Without tests, regressions can silently break the keepalive and autofix functionality.
   - The existing test patterns in __tests__/ provide a template but these new scripts are not covered.

   Goal:
   - Add comprehensive Jest test suites for all new agent scripts.
   - Achieve >80% code coverage for the critical path functions.
   - Include edge case testing for malformed inputs, API failures, and boundary conditions.

   Tasks:
   - Create .github/scripts/__tests__/keepalive-loop.test.js:
         - Test countCheckboxes() with various markdown formats
         - Test evaluateKeepaliveLoop() with mocked GitHub API responses
         - Test stop condition logic (tasks-complete, max-iterations, repeated-failure)
         - Test PR body parsing and config extraction
         - Test concurrency group name generation
   - Create .github/scripts/__tests__/keepalive-state.test.js:
         - Test loadKeepaliveState() with various comment formats
         - Test formatStateComment() output structure
         - Test state persistence round-trip (save → load → verify)
         - Test edge cases: missing comments, malformed JSON, empty PRs
   - Create .github/scripts/__tests__/agents-verifier-context.test.js:
         - Test buildVerifierContext() with mocked PR and issue data
         - Test extractScopeTasksAcceptanceSections() integration
         - Test linked issue resolution via GraphQL mock
         - Test skip conditions (non-merged PR, missing SHA, fork detection)
   - Update package.json test configuration if needed for coverage thresholds
   - Add test fixtures for realistic PR bodies and issue content

   Acceptance criteria:
   - All three test files exist and pass in CI.
   - Combined coverage for keepalive_loop.js, keepalive_state.js, and agents_verifier_context.js exceeds 80%.
   - Tests catch at least one real bug or edge case during development.
   - Test output is integrated into the existing Jest test run (npm test).

9. Add workflow execution metrics and observability dashboard

   Context / problem:
   - The agent workflows (keepalive-loop, autofix-loop, verifier) run frequently but there's no visibility into:
     - How many iterations each PR takes to complete
     - Success vs failure rates for autofix attempts
     - Which stop conditions are triggered most often
     - Average time from PR open to tasks-complete
     - Which types of failures autofix can vs cannot fix
   - Debugging loop behavior requires manually inspecting workflow run logs.
   - There's no historical trend data to identify regressions or improvements.

   Goal:
   - Add lightweight metrics collection to agent workflows.
   - Create a metrics summary that can be viewed in GitHub Actions job summaries.
   - Store historical data in a JSON artifact or repository file for trend analysis.

   Tasks:
   - Add metrics emission to keepalive-loop workflow:
         - Emit: pr_number, iteration_count, stop_reason, gate_conclusion, tasks_total, tasks_completed, duration_seconds
         - Write to GITHUB_STEP_SUMMARY in structured format
         - Append to metrics artifact (ndjson format)
   - Add metrics emission to autofix-loop workflow:
         - Emit: pr_number, attempt_number, trigger_reason (mypy/pytest/lint), fix_applied (bool), gate_result_after
         - Track which job/step triggered the autofix
   - Add metrics emission to verifier workflow:
         - Emit: pr_number, verdict (pass/fail), issues_created, acceptance_criteria_count, checks_run
   - Create a metrics aggregation script (scripts/aggregate_agent_metrics.py):
         - Read ndjson metrics files
         - Compute summary statistics (success rate, avg iterations, common stop reasons)
         - Output markdown summary suitable for weekly reports
   - Add a scheduled workflow (weekly) to:
         - Download recent metrics artifacts
         - Run aggregation script
         - Post summary to a tracking issue or wiki page
   - Add metrics visualization to PR comments:
         - Show iteration progress bar in keepalive summary comment
         - Show autofix attempt count and outcomes

   Acceptance criteria:
   - Each agent workflow run produces structured metrics in GITHUB_STEP_SUMMARY.
   - Metrics are persisted as artifacts for at least 30 days.
   - Weekly summary report is automatically generated and posted.
   - Metrics can answer: "What percentage of PRs complete via keepalive without human intervention?"